{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural-Networks-Complete-Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8Ays6cbG3w4GaXWZZEhAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helder-garcia/neural-networks-practice/blob/main/Neural_Networks_Complete_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfYj9H5mqgYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecae3ece-c009-4dfb-b56a-2485ef451244"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import dataloader\n",
        "\n",
        "args = {\n",
        "    'batch_size': 20,\n",
        "    'num_workers': 2\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzZC5jeFs9qP"
      },
      "source": [
        "training_set = datasets.KMNIST('./', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_set = datasets.KMNIST('./', train=False, download=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = dataloader.DataLoader(training_set, shuffle=True, batch_size=args['batch_size'], num_workers=args['num_workers'])\n",
        "test_loader = dataloader.DataLoader(test_set, shuffle=True, batch_size=args['batch_size'], num_workers=args['num_workers'])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI7UvaeBu1Fc"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, out_size):\n",
        "    super(MLP, self).__init__()\n",
        "    self.features  = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.out     = nn.Linear(hidden_size, out_size)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = X.view(X.size(0), -1)\n",
        "    feature = self.features(X)\n",
        "    output  = self.softmax(self.out(feature))\n",
        "    return output\n",
        "\n",
        "input_size  = 28 * 28\n",
        "hidden_size = 64\n",
        "out_size    = 10\n",
        "net = MLP(input_size, hidden_size, out_size).to(device)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjK4Tr3x0pmG"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-1, weight_decay=5e-4)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_9FIB0f4NOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82acfe9-543b-4beb-a515-764a5867f2e1"
      },
      "source": [
        "for epoch in range(30):\n",
        "  start = time.time()\n",
        "  epoch_loss = []\n",
        "  for data, label in train_loader:\n",
        "    data = data.to(device)\n",
        "    label = label.to(device)\n",
        "    # Forward\n",
        "    pred = net(data)\n",
        "    loss = criterion(pred, label)\n",
        "    epoch_loss.append(loss.cpu().data)\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "  end = time.time()\n",
        "  print(\"Epoca %d, Loss: %.8f +\\- %.4f, Tempo: %.2f\" % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start) )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoca 0, Loss: 2.34553385 +\\- 0.0802, Tempo: 14.94\n",
            "Epoca 1, Loss: 2.35456729 +\\- 0.0692, Tempo: 15.01\n",
            "Epoca 2, Loss: 2.35468411 +\\- 0.0711, Tempo: 14.92\n",
            "Epoca 3, Loss: 2.35468388 +\\- 0.0687, Tempo: 14.93\n",
            "Epoca 4, Loss: 2.35468411 +\\- 0.0697, Tempo: 14.87\n",
            "Epoca 5, Loss: 2.35470057 +\\- 0.0685, Tempo: 14.85\n",
            "Epoca 6, Loss: 2.35471749 +\\- 0.0699, Tempo: 14.77\n",
            "Epoca 7, Loss: 2.35473394 +\\- 0.0687, Tempo: 14.90\n",
            "Epoca 8, Loss: 2.35473394 +\\- 0.0689, Tempo: 14.99\n",
            "Epoca 9, Loss: 2.35473394 +\\- 0.0698, Tempo: 14.77\n",
            "Epoca 10, Loss: 2.35473394 +\\- 0.0704, Tempo: 14.94\n",
            "Epoca 11, Loss: 2.35473394 +\\- 0.0690, Tempo: 14.86\n",
            "Epoca 12, Loss: 2.35473394 +\\- 0.0692, Tempo: 14.83\n",
            "Epoca 13, Loss: 2.35473394 +\\- 0.0696, Tempo: 15.06\n",
            "Epoca 14, Loss: 2.35473394 +\\- 0.0681, Tempo: 14.96\n",
            "Epoca 15, Loss: 2.35473394 +\\- 0.0692, Tempo: 15.04\n",
            "Epoca 16, Loss: 2.35475063 +\\- 0.0682, Tempo: 15.08\n",
            "Epoca 17, Loss: 2.35475063 +\\- 0.0686, Tempo: 15.21\n",
            "Epoca 18, Loss: 2.35475087 +\\- 0.0694, Tempo: 15.14\n",
            "Epoca 19, Loss: 2.35475063 +\\- 0.0687, Tempo: 14.98\n",
            "Epoca 20, Loss: 2.35475087 +\\- 0.0693, Tempo: 15.02\n",
            "Epoca 21, Loss: 2.35475063 +\\- 0.0702, Tempo: 15.08\n",
            "Epoca 22, Loss: 2.35475063 +\\- 0.0698, Tempo: 14.95\n",
            "Epoca 23, Loss: 2.35475063 +\\- 0.0697, Tempo: 15.13\n",
            "Epoca 24, Loss: 2.35475087 +\\- 0.0685, Tempo: 15.03\n",
            "Epoca 25, Loss: 2.35475063 +\\- 0.0686, Tempo: 15.08\n",
            "Epoca 26, Loss: 2.35475063 +\\- 0.0687, Tempo: 14.95\n",
            "Epoca 27, Loss: 2.35475087 +\\- 0.0691, Tempo: 14.94\n",
            "Epoca 28, Loss: 2.35475087 +\\- 0.0683, Tempo: 15.02\n",
            "Epoca 29, Loss: 2.35475063 +\\- 0.0674, Tempo: 14.97\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}